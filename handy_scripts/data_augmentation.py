# -*- coding: utf-8 -*-
"""data_augmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PN72WHUz044yplHyMBm-oZDQBipNRpW3
"""

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow.keras import layers

"""#Tensorflow DataSet"""

# All dataset builders are subclass of tfds.core.DatasetBuilder. To get the list of available builders, uses tfds.list_builders()
tfds.list_builders()

"""## Load a dataset
The easiest way of loading a dataset is tfds.load. It will:

Download the data and save it as tfrecord files.
Load the tfrecord and create the tf.data.Dataset.

*By using **batch_size=-1**, you can load the **full dataset** in a single batch.*
"""

from tensorflow.python.data.ops.dataset_ops import Dataset
ds = tfds.load('tf_flowers', split='train', shuffle_files=True)
assert isinstance(ds, tf.data.Dataset)

print(ds)

"""##Iterate over a dataset

####As dict </p>

By default, the tf.data.Dataset object contains a dict of tf.Tensors:
"""

for example in ds.take(6):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`
  print(list(example.keys()))
  image = example["image"]
  label = example["label"]
  print(image.shape, label)

expl1 = ds.take(1)  # Only take a single example

item = expl1.as_numpy_iterator().next()

item = expl1.as_numpy_iterator().next()
print(item['label'])
plt.imshow(item['image'])

"""####As tuple

By using **as_supervised=True**, you can get a tuple (features, label) instead for supervised datasets.
"""

ds = tfds.load('tf_flowers', split='train', shuffle_files=True, as_supervised=True)

item_tuple = ds.take(1).as_numpy_iterator().next()
print(item_tuple[1])
plt.imshow(item_tuple[0])

for image, label in ds.take(6):
  print(f'shape of image {image.shape}')
  print(f'label is {label}')

"""### As numpy array

Uses tfds.as_numpy to convert:

tf.Tensor -> np.array
tf.data.Dataset -> Generator[np.array]

"""

for image, label in tfds.as_numpy(ds.take(6)):
  print(type(image), type(label), label)

"""## Visualize a dataset and get info of dataset

Visualize datasets with tfds.show_examples (only image datasets supported now):
"""

ds, info = tfds.load('tf_flowers', split = 'train', with_info=True)

fig = tfds.show_examples(ds, info)

info

info.features

info.features['label'].names

info.features['label'].num_classes

info.splits['train'].num_examples

info.splits['test'].num_examples

print(list(info.splits.keys()))

"""#Augmentation"""

(train_ds, val_ds, test_ds), metadata = tfds.load(
    'tf_flowers',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

metadata.features['label'].names

metadata.features['label'].num_classes

get_label_name = metadata.features['label'].int2str

image, label = next(iter(train_ds.take(1)))
print(get_label_name(label))
plt.imshow(image)

"""## Use Keras preprocessing layers

### Resizing and rescaling
"""

IMG_SIZE = 180

resize_rescale = tf.keras.models.Sequential((
    layers.Resizing(IMG_SIZE, IMG_SIZE),
    layers.Rescaling(1./255)
))

rezult = resize_rescale(image)
plt.imshow(rezult)

"""### Flip, Rotation"""

flip_rotation = tf.keras.models.Sequential((
    layers.RandomFlip(),
    layers.RandomRotation(0.3)
))

rezult = resize_rescale(image)
rezult = flip_rotation(rezult)
plt.imshow(rezult)

"""## Two options to use the Keras preprocessing layers
1. **Option 1: Make the preprocessing layers part of your model**

**Note**: Data augmentation is inactive at test time so input images will only be augmented during calls to Model.fit (not Model.evaluate or Model.predict).

2. **Option 2: Apply the preprocessing layers to your dataset**
"""

# option 2
aug_ds = train_ds.map(
  lambda x, y: (resize_rescale(x, training=True), y))

"""### Build model using augmentation data"""

batch_size=32

AUTOTUNE = tf.data.AUTOTUNE

def prepare_data(ds, shuffle=False, augment = False):
  ds = ds.map(lambda x,y: (resize_rescale(x, training = True),y), num_parallel_calls=AUTOTUNE)
  if shuffle:
    ds.shuffle(1000)
  if augment:
    ds.map(lambda x,y: (flip_rotation(x, training = True),y), num_parallel_calls=AUTOTUNE)
  # Batch all datasets.

  ds = ds.batch(batch_size)

  # Use buffered prefetching on all datasets.
  return ds.prefetch(buffer_size=AUTOTUNE)

train_ds = prepare_data(train_ds, shuffle=True, augment=True)
val_ds = prepare_data(val_ds)
test_ds = prepare_data(test_ds)

num_classes = metadata.features['label'].num_classes
num_classes

model = tf.keras.Sequential([
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes, activation = 'softmax')
])

model.compile(optimizer='Adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(), metrics= ['accuracy'])

model.fit(train_ds, epochs = 10, validation_data = val_ds)

"""### Custom data augmentation

1. tf.keras.layers.Lambda.

2. new layer via subclassing



"""

def random_invert_image(x, p = 0.7):
  if tf.random.uniform([]) < p:
    x = 255-x
  return x

def random_invert(factor = 0.7):
  return layers.Lambda(lambda x: random_invert_image(x, factor))

augmented_image = random_invert()(image)
plt.imshow(augmented_image)
plt.axis("off")

class RandomInvert(tf.keras.layers.Layer):
  def __init__(self, factor=0.4, **kwargs):
    super().__init__(**kwargs)
    self._factor = factor

  def call(self, x):
    return random_invert_image(x, self._factor)

_ = plt.imshow(RandomInvert(0.7)(image))

